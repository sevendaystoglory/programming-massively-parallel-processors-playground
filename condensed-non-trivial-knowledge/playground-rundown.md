following are the kernels inplemented in the playground:
1. [vecaddlarge](../playground/vecaddsingle.cu) to add two vectors using a single threadblock. 
2. [vecaddlarge](../playground/vecaddlarge.cu) to add two 1GiB vectors using multiple threadblocks. ~4% GPU util. ~3.3GiB memory. we really can't do shite about it because adding two vectors is fundamentally a memory bound task. you have to move 12 bytes per operation. we can however increase the blockDim to 1024 and let each CUDA core manage context swtich. this is called latency tolerance. we could also have each kernel run longer by reducing the gridDim to impose at max 16 blocks per SM. this won't save latency at all just that each kernel would now fill more C[i] sequentially.
3. [matrixadd.cu](../playground/matrixadd.cu) to add two marices. We convert everything to row major layout before pushing to CUDA and kernel prefers wo do so.
4. [matmul.cu](../playground/matmul.cu) to multiply two matrices. Being an $O(n^3)$ operation we can multiply pretty quick. If we look at the kernel it performs N operations for each i, j index of the final matrix C. What this means is that each thread picks up a row and a column from the matrices and do element-wise addition on top of that [A]. Thus, the duration of the thread gets longer as we scale matrices. Also, again we are doing 1 operation for each 12 bytes of data moved. And it takes 0.002053s for N = 1<<14. [B]

[4.A]: The consequence of this is that for the second matrix where all the column elements are fetched, we are performing inefficient [uncoalesced memory accesses](https://youtu.be/XEOc4HCf_pQ?si=hANJ7Qw_RklEgM30&t=470).  