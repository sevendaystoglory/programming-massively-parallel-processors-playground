following are the kernels inplemented in the playground:
1. [vecaddlarge](../playground/vecaddsingle.cu) to add two vectors using a single threadblock. 
2. [vecaddlarge](../playground/vecaddlarge.cu) to add two 1GiB vectors using multiple threadblocks. ~4% GPU util. ~3.3GiB memory. we really can't do shite about it because adding two vectors is fundamentally a memory bound task. you have to move 12 bytes per operation. we can however increase the blockDim to 1024 and let each CUDA core manage context swtich. this is called latency tolerance. we could also have each kernel run longer by reducing the gridDim to impose at max 16 blocks per SM. this won't save latency at all just that each kernel would now fill more C[i] sequentially.
3. [matrixadd.cu](../playground/matrixadd.cu) to add two marices. We convert everything to row major layout before pushing to CUDA and kernel prefers wo do so.
4. [matmul.cu](../playground/matmul.cu) to multiply two matrices. Being an $O(n^3)$ operation we can multiply pretty quick. If we look at the kernel it performs N operations for each i, j index of the final matrix C. What this means is that each thread picks up a row and a column from the matrices and do element-wise addition on top of that [A]. Thus, the duration of the thread gets longer as we scale matrices. Also, again we are doing 1 operation for each 12 bytes of data moved. And it takes **0.002053s** for N = 1<<14 [B]. We can improve upon this naive implementation by two optimizations.

[4.A]-OPTIMIZATION 1 : Coalescing: Whenever the second matrix where all the column elements are fetched, we are performing inefficient [uncoalesced memory accesses](https://youtu.be/XEOc4HCf_pQ?si=hANJ7Qw_RklEgM30&t=470). We solve this by converting B to a column major layout. It takes **17.05ms** now. Check out the mod. 
[4.B]-OPTIMIZATION 2: Tiled Matrix Mutliplication: We are going to use the fact that each element of either A or B is used N times, however we are fetching it exactly N times from the DRAM :( Only if we could store it somewhere in the L1 cache :) Each thread in my block (kxk, let) fetches each corresponding element from tiles on A and B. Also, we no longer fetch an entire row/column per thread from the global memory. What we do is let each thread first move the tiles to shared memory so that each thread could use these elements again in the kernel. It takes **3.80ms.**. We are now x5 faster! Check out [matmultiled.cu](../playground/matmultiled.cu).

OPTIMIZATION 3: Using Pinned Memory: Using cudaMallocHost will reserve pinned memory (locks pages in RAM so they can't be swapped out) in the CPU RAM so that the operation cudaMemcpy is faster.

5. [basicraytracer.cu](../playground/basicraytracer.cu) implements a simple orthogonal raytracer kernel. We also get to work with constant memory and shave off 30% of rendering time. I've plans on how to procees with improving the kernel. Here it goes.
5.1: Image Quality -- 5.1.a: add perspective camera; 5.1.a: simple phong shading; 5.1.c hard shadows; 5.1.d anti-aliasing.
5.2: Speed Optimizations -- coalesing AoS to SoA, __launch_bounds__, measure occupancy and memory stalls w/ nsight compute.
5.3: Acceleration -- build a BVH, traverse the BVH instead of traversing every sphere.

6. [atomichistogram.cu](../playground/atomichistogram.cu) implements a function (both for a CPU runtime and CUDA runtime) that counts the number of each unsigned char in a 100M long buffer. We get a histogram of 256 bins. To prevent race conditions in the CUDA kernel, we use atomicAdd(&hist[buffer[idx]], 1). But the GPU takes 0.4s while the CPU takes 0.27s. Why? When thousands of thread are trying to read from a bin, great contentions can occur. The hardware will thus serialize these operations to a long waiting list. The reason of this slowdown is not atomicity but the humongous contentios memory acceses. We can solve this by allocating __shared__ memory for each block and then merging at the end of each thread doing its work in the block. We achieve x100 speedup using this. Check the mods branch.

7. Revisiting the matmultiled operation. Read Simon's blog and apparently we are not hitting good arithmetic operational intensity with our kernel. (See his blog - kernel 5 : 2D tiled matmul using a single thread.) Lets turn our memory bound kernel to a compute bound kernel. How? Lets assign more work per thread.
Some key points in our code:
A. We are using scalar accumulator i.e. int value to store the value of an element of C at thread level. This memory is allocated in the core's (or thread's) register.
B. Consequesntly to A, we are using #threads in a block  == #elements in a block tile.
C. Each thread is writing to one and only one element of C. The arithmetic intensity is very low. We can fix this by making each thread responsible for an 8x8 microtile of elements on C. This way we should be able to load the requisite microtiles of A and B in register cache and compute those elements. Arithmetic intensity FLOPS / BYTE:
- To compute an 8x8 microtile of C, we can have 8x8 microtiles of A and B. For each 2 elements moved(loaded to SMEM) we do 2 FLOPS. This is a totol of 0.25 FLOPS / BYTE. [Computed by 8x8 threads]
- What if I instead allocate 8 threads only to compute 8 outer products of 8 columns (not ROWS!) of A and 8 rows of B. This way we would do 128 FLOPS for 16 elements loaded (into SMEM/regs). This is a total of 2 FLOPS / BYTE. [Computed by 8 threads]
**This is the core idea of SGEMM Kernel design. The outer-product formulation (accumulating microtiles using shared sub-vectors of A and B) leads to much higher reuse per memory load and much better performance.**
What we've done is successfully relieved memory stress from our task by trading in for a more compute-bound implementation. See [registeredcached_matmul_withblocktiling](../playground/registercached_matmul_withblocktiliing.cu). Takes 3.1ms. (However I believe we've let go of some coalescing gains). If we do something like `#pragma unroll` above a fixed length for loop in the kernel - we improve to ~ 2.2ms.
