following are the kernels inplemented in the playground:
1. [vecaddlarge](../playground/vecaddsingle.cu) to add two vectors using a single threadblock. 
2. [vecaddlarge](../playground/vecaddlarge.cu) to add two 1GiB vectors using multiple threadblocks. ~4% GPU util. ~3.3GiB memory. we really can't do shite about it because adding two vectors is fundamentally a memory bound task. you have to move 12 bytes per operation. we can however increase the blockDim to 1024 and let each CUDA core manage context swtich. this is called latency tolerance. we could also have each kernel run longer by reducing the gridDim to impose at max 16 blocks per SM. this won't save latency at all just that each kernel would now fill more C[i] sequentially.
3. [matrixadd.cu](../playground/matrixadd.cu) to add two marices. We convert everything to row major layout before pushing to CUDA and kernel prefers wo do so.
4. [matmul.cu](../playground/matmul.cu) to multiply two matrices. Being an $O(n^3)$ operation we can multiply pretty quick. If we look at the kernel it performs N operations for each i, j index of the final matrix C. What this means is that each thread picks up a row and a column from the matrices and do element-wise addition on top of that [A]. Thus, the duration of the thread gets longer as we scale matrices. Also, again we are doing 1 operation for each 12 bytes of data moved. And it takes **0.002053s** for N = 1<<14 [B]. We can improve upon this naive implementation by two optimizations.

[4.A]-OPTIMIZATION 1 : Coalescing: Whenever the second matrix where all the column elements are fetched, we are performing inefficient [uncoalesced memory accesses](https://youtu.be/XEOc4HCf_pQ?si=hANJ7Qw_RklEgM30&t=470). We solve this by converting B to a column major layout. It takes **0.001705s** now. Check out the mod. 
[4.B]-OPTIMIZATION 2: Tiled Matrix Mutliplication: We are going to use the fact that each element of either A or B is used N times, however we are fetching it exactly N times from the DRAM :( Only if we could store it somewhere in the L1 cache :) Each thread in my block (kxk, let) fetches each corresponding element from tiles on A and B. Also, we no longer fetch an entire row/column per thread from the global memory. What we do is let each thread first move the tiles to shared memory so that each thread could use these elements again in the kernel. It takes **0.000380s.** Check out [matmultiled.cu](../playground/matmultiled.cu).